{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2cc716",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "Using the dataset, create Lasso regression models to predict the ‘mpg’ for a car \n",
    "using 20% and 80% of the training data for the 2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "555d957f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 9)\n",
      "   Unnamed: 0   mpg  cylinders  displacement  horsepower  weight  \\\n",
      "0           0  18.0          8         307.0       130.0    3504   \n",
      "1           1  15.0          8         350.0       165.0    3693   \n",
      "2           2  18.0          8         318.0       150.0    3436   \n",
      "3           3  16.0          8         304.0       150.0    3433   \n",
      "4           4  17.0          8         302.0       140.0    3449   \n",
      "\n",
      "   acceleration  model year  origin  \n",
      "0          12.0          70       1  \n",
      "1          11.5          70       1  \n",
      "2          11.0          70       1  \n",
      "3          12.0          70       1  \n",
      "4          10.5          70       1  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#We will split the data into two - 20% and 80%. \n",
    "#We shall use each of these to test and train each other models\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cars = pd.read_csv(\"assignment4.csv\")\n",
    "\n",
    "#Getting the dimensions of the dataset\n",
    "print(cars.shape)\n",
    "\n",
    "print(cars.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "48088104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R squared for 80% trained model is 0.7989\n",
      "\n",
      "The MSE for 80% trained model is 12.2428\n",
      "\n",
      "The coefficients for 80% model are [ 0.          0.         -0.00775594 -0.00656387  0.          0.64923279\n",
      "  0.        ]\n",
      "\n",
      "The OOS R squared for 80% trained model is 0.8336\n",
      "\n",
      "\n",
      "The R squared for 20% trained model is 0.8413\n",
      "\n",
      "The MSE for 20% trained model is 9.7054\n",
      "\n",
      "The coefficients for 20% model are [-0.          0.          0.00893342 -0.00677418  0.04029953  0.7460807\n",
      "  0.        ]\n",
      "\n",
      "The OOS R squared for 20% trained model is 0.7986\n"
     ]
    }
   ],
   "source": [
    "#Dropping the first column as it is not necessary\n",
    "\n",
    "try:\n",
    "    cars = cars.drop(columns = ['Unnamed: 0'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Creating the training data\n",
    "\n",
    "# Splitting the data into 80-20 chunks\n",
    "split_size=0.8\n",
    "\n",
    "X = cars.drop(columns = ['mpg']).copy()\n",
    "y = cars['mpg']\n",
    "\n",
    "#Splitting the data into two chunks\n",
    "X_80, X_20, y_80, y_20 = train_test_split(X,y, train_size=split_size, random_state= 75)\n",
    "\n",
    "#print(X_80.shape)\n",
    "#print(X_20.shape)\n",
    "#print(y_80.shape)\n",
    "#print(y_20.shape)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score #For calculating R-squared\n",
    "from sklearn.metrics import mean_squared_error #For calculating MSE\n",
    "##################################################################\n",
    "\n",
    "#Fitting the model with 80% train data\n",
    "\n",
    "#Defining the Lasso model with lambda (alpha in the method) = 1\n",
    "\n",
    "model_80train_20test = Lasso(alpha=1.0)\n",
    "\n",
    "model_80train_20test.fit(X_80,y_80)\n",
    "\n",
    "#Predicting the output with the Out of sample data\n",
    "y_80_predicted = model_80train_20test.predict(X_80)\n",
    "\n",
    "#Calculating the R-squared\n",
    "r2_80train_20_test = r2_score(y_80, y_80_predicted)\n",
    "\n",
    "#Calculating the MSE\n",
    "mse_80train_20_test = mean_squared_error(y_80, y_80_predicted)\n",
    "\n",
    "\n",
    "###################################################################\n",
    "\n",
    "#Fitting the model with 20% train data\n",
    "    \n",
    "#Defining the Lasso model with lambda (alpha in the method) = 1\n",
    "\n",
    "model_20train_80test = Lasso(alpha=1.0)\n",
    "\n",
    "model_20train_80test.fit(X_20,y_20)\n",
    "\n",
    "#Predicting the output with the Out of sample data\n",
    "y_20_predicted = model_20train_80test.predict(X_20)\n",
    "\n",
    "#Calculating the Out-of-sample R-squared\n",
    "r2_20train_80_test = r2_score(y_20, y_20_predicted)\n",
    "\n",
    "#Calculating the Out-of-sample MSE\n",
    "mse_20train_80_test = mean_squared_error(y_20, y_20_predicted)\n",
    "\n",
    "\n",
    "print(\"The R squared for 80% trained model is\", round(r2_80train_20_test,4))\n",
    "print(\"\\nThe MSE for 80% trained model is\", round(mse_80train_20_test,4))\n",
    "print(\"\\nThe coefficients for 80% model are\", model_80train_20test.coef_)\n",
    "print(\"\\nThe OOS R squared for 80% trained model is\", round(r2_score(y_20, model_80train_20test.predict(X_20)),4))\n",
    "\n",
    "print(\"\\n\\nThe R squared for 20% trained model is\", round(r2_20train_80_test,4))\n",
    "print(\"\\nThe MSE for 20% trained model is\", round(mse_20train_80_test,4))\n",
    "print(\"\\nThe coefficients for 20% model are\", model_20train_80test.coef_)\n",
    "print(\"\\nThe OOS R squared for 20% trained model is\", round(r2_score(y_80, model_20train_80test.predict(X_80)),4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a0861",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "For the above model, tune ‘alpha’ using the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to find the optimum value for it. Explain how that value for alpha was chosen. Build a model using that value of alpha.\n",
    "\n",
    "As we see, the model with 80% training data has better OOS R squared as well as MSE. Hence we will choose this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8771b1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_bddff_row5_col0,#T_bddff_row5_col1{\n",
       "            font-weight:  bold;\n",
       "        }</style><table id=\"T_bddff_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >AIC criterion</th>        <th class=\"col_heading level0 col1\" >BIC criterion</th>    </tr>    <tr>        <th class=\"index_name level0\" >alphas</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_bddff_level0_row0\" class=\"row_heading level0 row0\" >5396.386629286816</th>\n",
       "                        <td id=\"T_bddff_row0_col0\" class=\"data row0 col0\" >318.000000</td>\n",
       "                        <td id=\"T_bddff_row0_col1\" class=\"data row0 col1\" >318.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_bddff_level0_row1\" class=\"row_heading level0 row1\" >25.612232052031285</th>\n",
       "                        <td id=\"T_bddff_row1_col0\" class=\"data row1 col0\" >101.785670</td>\n",
       "                        <td id=\"T_bddff_row1_col1\" class=\"data row1 col1\" >105.547722</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_bddff_level0_row2\" class=\"row_heading level0 row2\" >15.803221689276588</th>\n",
       "                        <td id=\"T_bddff_row2_col0\" class=\"data row2 col0\" >102.623031</td>\n",
       "                        <td id=\"T_bddff_row2_col1\" class=\"data row2 col1\" >110.147133</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_bddff_level0_row3\" class=\"row_heading level0 row3\" >8.810170762726942</th>\n",
       "                        <td id=\"T_bddff_row3_col0\" class=\"data row3 col0\" >102.471467</td>\n",
       "                        <td id=\"T_bddff_row3_col1\" class=\"data row3 col1\" >113.757621</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_bddff_level0_row4\" class=\"row_heading level0 row4\" >2.532008485512097</th>\n",
       "                        <td id=\"T_bddff_row4_col0\" class=\"data row4 col0\" >72.223262</td>\n",
       "                        <td id=\"T_bddff_row4_col1\" class=\"data row4 col1\" >83.509416</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_bddff_level0_row5\" class=\"row_heading level0 row5\" >0.8389715747340596</th>\n",
       "                        <td id=\"T_bddff_row5_col0\" class=\"data row5 col0\" >69.826350</td>\n",
       "                        <td id=\"T_bddff_row5_col1\" class=\"data row5 col1\" >81.112504</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_bddff_level0_row6\" class=\"row_heading level0 row6\" >0.4436366205191914</th>\n",
       "                        <td id=\"T_bddff_row6_col0\" class=\"data row6 col0\" >71.591201</td>\n",
       "                        <td id=\"T_bddff_row6_col1\" class=\"data row6 col1\" >86.639406</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_bddff_level0_row7\" class=\"row_heading level0 row7\" >0.08902377684326214</th>\n",
       "                        <td id=\"T_bddff_row7_col0\" class=\"data row7 col0\" >70.688751</td>\n",
       "                        <td id=\"T_bddff_row7_col1\" class=\"data row7 col1\" >89.499008</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_bddff_level0_row8\" class=\"row_heading level0 row8\" >0.04761791623217235</th>\n",
       "                        <td id=\"T_bddff_row8_col0\" class=\"data row8 col0\" >72.591364</td>\n",
       "                        <td id=\"T_bddff_row8_col1\" class=\"data row8 col1\" >95.163672</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_bddff_level0_row9\" class=\"row_heading level0 row9\" >0.0</th>\n",
       "                        <td id=\"T_bddff_row9_col0\" class=\"data row9 col0\" >74.493667</td>\n",
       "                        <td id=\"T_bddff_row9_col1\" class=\"data row9 col1\" >100.828027</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff2f33b6550>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler #For standardizing\n",
    "#I will not use in this example\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC #For tuning alpha (lambda)\n",
    "from sklearn.pipeline import make_pipeline \n",
    "\n",
    "import time #Optional\n",
    "\n",
    "lasso_lars_ic = make_pipeline(\n",
    "    LassoLarsIC(criterion=\"aic\", normalize=False)\n",
    ").fit(X_80, y_80)\n",
    "\n",
    "\n",
    "#Calculating AIC for alphas\n",
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"alphas\": lasso_lars_ic[-1].alphas_,\n",
    "        \"AIC criterion\": lasso_lars_ic[-1].criterion_,\n",
    "    }\n",
    ").set_index(\"alphas\")\n",
    "alpha_aic = lasso_lars_ic[-1].alpha_\n",
    "\n",
    "#Calculating BIC for alphas\n",
    "\n",
    "lasso_lars_ic.set_params(lassolarsic__criterion=\"bic\").fit(X_80, y_80) #Changing to bic\n",
    "results[\"BIC criterion\"] = lasso_lars_ic[-1].criterion_\n",
    "alpha_bic = lasso_lars_ic[-1].alpha_\n",
    "\n",
    "#Putting the results in a tabular form\n",
    "def highlight_min(x):\n",
    "    x_min = x.min()\n",
    "    return [\"font-weight: bold\" if v == x_min else \"\" for v in x]\n",
    "\n",
    "\n",
    "results.style.apply(highlight_min)\n",
    "\n",
    "#reference: https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74971d88",
   "metadata": {},
   "source": [
    "As we see, for an alpha of 0.8389, both the AIC and the BIC are the least. This indicates that the model has the least in-sample deviance for a hyperparameter value of 0.8389.\n",
    "\n",
    "For each increasing value of alpha, the penalty function increases - indicating that lesser variables have a chance to get selected. So when we tune alpha using AIC or BIC, we select a value that gives us the minimum \"deviance from truth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "aa917d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The coefficients for the model with low AIC/BIC are [ 0.00000000e+00  8.42099254e-06 -7.44427408e-03 -6.55919900e-03\n",
      "  0.00000000e+00  6.62518239e-01  0.00000000e+00]\n",
      "\n",
      "The out of sample R-square for the model with lowest AIC/BIC is 0.7993\n"
     ]
    }
   ],
   "source": [
    "#Building a model with the chosen value of alpha\n",
    "\n",
    "#Defining the Lasso model with lambda (alpha in the method) = 0.8389\n",
    "\n",
    "model_low_aic_bic = Lasso(alpha= 0.8389715747340596)\n",
    "\n",
    "model_low_aic_bic.fit(X_80,y_80)\n",
    "print(\"\\nThe coefficients for the model with low AIC/BIC are\", model_low_aic_bic.coef_)\n",
    "\n",
    "#This is the model with the new hyperparameter alpha\n",
    "\n",
    "predicted_y_low_aic_bic = model_low_aic_bic.predict(X_80)\n",
    "r2_low_aic_bic = r2_score(y_80, predicted_y_low_aic_bic)\n",
    "\n",
    "#Calculating the Out-of-sample MSE\n",
    "mse_low_aic_bic = mean_squared_error(y_80, predicted_y_low_aic_bic)\n",
    "\n",
    "#Calculating the SSE\n",
    "sse_low_aic_bic = mse_low_aic_bic*len(y_80)\n",
    "\n",
    "print(\"\\nThe out of sample R-square for the model with lowest AIC/BIC is\", round(r2_low_aic_bic,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd33ee6",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "Build a simple regression model using 10-fold cross validation for the same data. Write your observations about the R-squared values for the models and their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2a52f0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared for the Cross Validation model is 0.8092\n"
     ]
    }
   ],
   "source": [
    "#For cross validation, we will use the entire data set instead of the 80-20 split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "#Defining the CV method to use. 10-fold\n",
    "cv = KFold(n_splits=10, random_state=28, shuffle=True)\n",
    "\n",
    "#Defining a linear regression model\n",
    "model_cv = LinearRegression()\n",
    "\n",
    "#use k-fold CV to evaluate model\n",
    "cv_r_squares = cross_val_score(model_cv, X, y, scoring='r2',\n",
    "                         cv=cv, n_jobs=-1)\n",
    "print(\"The R-squared for the Cross Validation model is\",round(np.mean(cv_r_squares),4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1adfc",
   "metadata": {},
   "source": [
    "The linear regression with cross validation has an R-squared of 80.92%.\n",
    "\n",
    "The Out of Sample R squared value for the 80% trained model Lasso (alpha = 1) is 83.36% and that of the 20% trained model is 79.89%.\n",
    "\n",
    "This shows that in this case, Lasso model with alpha = 1 and 80% training data has the highest out of sample R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94612ec",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Calculate AIC, AICc (i.e. Corrected AIC) and BIC for the models you built in question 1 and question 2. According to each of the measures, which is the better model? Is BIC always greater than AIC? Please explain. Compare the AICs with the corresponding corrected AICs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "07694ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'AIC': 802.5706613185764}, {'BIC': 813.8568154669169}, {'AICc': 802.6470944395955})\n",
      "({'AIC': 189.81491600472398}, {'BIC': 199.34302254341952}, {'AICc': 190.34824933805731})\n",
      "({'AIC': 803.950392058235}, {'BIC': 818.9985975893558}, {'AICc': 804.0781875853916})\n"
     ]
    }
   ],
   "source": [
    "#Following are the 3 models built in Questions 1 & 2\n",
    "\n",
    "#model_80train_20test\n",
    "#model_20train_80test\n",
    "#model_low_aic_bic\n",
    "\n",
    "def info_criteria(n, mse, num_params):\n",
    "    aic = n * log(mse) + 2 * num_params\n",
    "    aic_c = aic + (2*num_params*(num_params+1))/(n - num_params - 1)\n",
    "    bic = n * log(mse) + log(n) * num_params\n",
    "    return ({\"AIC\":aic}, {\"BIC\":bic}, {\"AICc\" : aic_c})\n",
    "\n",
    "\n",
    "#Model with 80% train data\n",
    "print(info_criteria(len(X_80), mse_80train_20_test, 3))\n",
    "\n",
    "#Model with 20% train data\n",
    "print(info_criteria(len(X_20), mse_20train_80_test, 4))\n",
    "\n",
    "#Model selected after tuning alpha\n",
    "print(info_criteria(len(X_80), mse_low_aic_bic, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a37a4",
   "metadata": {},
   "source": [
    "Although the AIC, BIC and AICc for the second model (With 20% train data) are lesser, we cannot compare these with the values for the other two models as the data used for training the 80% models is different from the one used for training the 20% model.\n",
    "\n",
    "Information Criteria cannot be compared between models built from different datasets.\n",
    "\n",
    "In the two models using the 80% train data, the model with alpha = 1 gives a better BIC (of 813.85) compared to the model with alpha = 0.83 (BIC = 818.99). Hence, with this training set, it is better to choose a hyperparameter (alpha) = 1. The AIC and AICc values are lower as well.\n",
    "\n",
    "The values of BIC are greater than AIC because BIC penalizes addition of new parameters more strongly compared to AIC. BIC is parsimony sensitive. AIC always has a chance of choosing too big a model, regardless of number of rows. BIC has very little chance of choosing too big a model if number of variables is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model. Hence, the penalty for choosing a larger model for BIC is higher than that of AIC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4bd790",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "What are Randomized Control Trials: A/B Tests? What is its significance? Elaborate. Explain methods used in analyzing the RCT data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68683fe8",
   "metadata": {},
   "source": [
    "In Randomized Control Trials, two or more parts of the population are randomly picked and a treatment (which is to be tested) is applied to one of the parts. \n",
    "\n",
    "A/B tests are widely used in multiple domains, especially in marketing analytics. They are used by companies to test new features in their websites when they cannot afford to roll it out to the whole population. They are also used to test two different types of creatives in the advertising domain.\n",
    "\n",
    "The most important aspect before performing an RCT is to ensure that the two populations are randomly selected as not randomizing will affect the results. The treatment effect is given by the difference in the response of interest between the population receiving the treatment versus the population not receiving the treatment.\n",
    "\n",
    "This difference will help estimate the actual impact that the treatment had on the population when compared to the control.\n",
    "\n",
    "Active Learning (AL) one of the methods that is used in analyzing RCT data. In AL, the outcome variable is constantly monitored for the RCT across all the treatments and the treatments giving the best results are given more weightage than the others. This enables quick selection of the best performing treatment. This is also known as Multi Armed bandits - and is an extremely popular technique used in marketing.\n",
    "\n",
    "Block-Design Experiment is also a method of performing and analyzing RCT data. In this design, the population is divided into blocks based on external variables (https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-32833-1_344). Each of these block receives the treatment and the control at some point. The treatment effect is then calculated by summing the differences in the outcome for treated and untreated across all blocks and then taking the average of this. This treatment effect can be interpreted as a causal effect as by using the block-design, other variables have been controlled for."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
